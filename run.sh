nohup python -u train.py model=blank_model model.name_or_path=/data1/llms/Llama-3.2-3B-Instruct/  model.block_name=LlamaDecoderLayer datasets=[pp_all] loss=sft exp_name=Llama-3.2-3B-Instruct gradient_accumulation_steps=2 batch_size=32 eval_batch_size=16 trainer=FSDPTrainer sample_during_eval=false > target5_pp_all.txt
nohup python -u train.py model=blank_model model.name_or_path=/data1/llms/Llama-3.2-3B-Instruct/  model.block_name=LlamaDecoderLayer datasets=[weight_pp] loss=sft exp_name=Llama-3.2-3B-Instruct gradient_accumulation_steps=2 batch_size=32 eval_batch_size=16 trainer=FSDPTrainer sample_during_eval=false > target5_weight_pp.txt
# nohup python -u train.py model=blank_model model.name_or_path=/data1/llms/Llama-3.2-3B-Instruct/  model.block_name=LlamaDecoderLayer datasets=[weight_pp_reason] loss=sft exp_name=Llama-3.2-3B-Instruct gradient_accumulation_steps=2 batch_size=32 eval_batch_size=16 trainer=FSDPTrainer sample_during_eval=false > target6_weight_pp_reason.txt
# nohup python -u train.py model=blank_model model.name_or_path=/data1/llms/Llama-3.2-3B-Instruct/  model.block_name=LlamaDecoderLayer datasets=[pp_small] loss=sft exp_name=Llama-3.2-3B-Instruct gradient_accumulation_steps=2 batch_size=32 eval_batch_size=16 trainer=FSDPTrainer sample_during_eval=false > target6_pp_small.txt
# nohup python -u train.py model=blank_model model.name_or_path=/data1/llms/Llama-3.2-3B-Instruct/  model.block_name=LlamaDecoderLayer datasets=[pp_all_reason] loss=sft exp_name=Llama-3.2-3B-Instruct gradient_accumulation_steps=2 batch_size=32 eval_batch_size=16 trainer=FSDPTrainer sample_during_eval=false > target6_pp_all_reason.txt
nohup python -u train.py model=blank_model model.archive=/data1/llms/sft-5/dpo/policy.pt  model.name_or_path=/data1/llms/Llama-3.2-3B-Instruct/ model.block_name=LlamaDecoderLayer datasets=[pp_all] loss=dpo loss.beta=0.1 exp_name=Llama-3.2-3B-Instruct gradient_accumulation_steps=2 batch_size=32 eval_batch_size=16 trainer=FSDPTrainer sample_during_eval=false > dpo_5.txt
nohup python -u train.py model=blank_model model.archive=/data1/llms/sft-5/wdpo/policy.pt  model.name_or_path=/data1/llms/Llama-3.2-3B-Instruct/ model.block_name=LlamaDecoderLayer datasets=[weight_pp] loss=wdpo loss.beta=0.1 exp_name=Llama-3.2-3B-Instruct gradient_accumulation_steps=2 batch_size=32 eval_batch_size=16 trainer=FSDPTrainer sample_during_eval=false > wdpo_5.txt
# nohup python -u train.py model=blank_model model.archive=/data1/llms/sft1/wdpo-reason/LATEST/policy.pt  model.name_or_path=/data1/llms/Llama-3.2-3B-Instruct/ model.block_name=LlamaDecoderLayer datasets=[weight_pp_reason] loss=wdpo loss.beta=0.1 exp_name=Llama-3.2-3B-Instruct gradient_accumulation_steps=2 batch_size=32 eval_batch_size=16 trainer=FSDPTrainer sample_during_eval=false > wdpo_reason.txt
# nohup python -u train.py model=blank_model model.archive=/data1/llms/sft/dpo-reason-small/policy.pt  model.name_or_path=/data1/llms/Llama-3.2-3B-Instruct/ model.block_name=LlamaDecoderLayer datasets=[dpo-reason-small] loss=dpo loss.beta=0.1 exp_name=Llama-3.2-3B-Instruct gradient_accumulation_steps=2 batch_size=32 eval_batch_size=16 trainer=FSDPTrainer sample_during_eval=false > wdpo_reason_small.txt
# nohup python -u train.py model=blank_model model.archive=/data1/llms/sft/dpo-all-reason/policy.pt  model.name_or_path=/data1/llms/Llama-3.2-3B-Instruct/ model.block_name=LlamaDecoderLayer datasets=[pp_all_reason] loss=dpo loss.beta=0.1 exp_name=Llama-3.2-3B-Instruct gradient_accumulation_steps=2 batch_size=32 eval_batch_size=16 trainer=FSDPTrainer sample_during_eval=false > dpo_reason_all.txt